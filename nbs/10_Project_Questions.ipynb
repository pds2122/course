{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pds2122/course/blob/main/nbs/10_Project_Questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G5_l2PDV9P6"
      },
      "source": [
        "<div class='bar_title'></div>\n",
        "\n",
        "*Practical Data Science*\n",
        "\n",
        "# Capstone Project Questions\n",
        "\n",
        "Matthias Griebel<br>\n",
        "Chair of Information Systems and Management\n",
        "\n",
        "Winter Semester 21/22"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoDbpryJV9QB"
      },
      "source": [
        "__Credits for this lecuture__\n",
        "\n",
        "- https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHQW_0OqV9QC"
      },
      "outputs": [],
      "source": [
        "# Install on colab\n",
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUnxqCGMXBHS"
      },
      "source": [
        "# Fine Tuning Transformer for MultiClass Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTvMQttvXBHS"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "Today, we will be fine tuning a transformer model for the **Multiclass text classification** problem.\n",
        "\n",
        " - Data: \n",
        "\t - Capstone Project Data.\n",
        "\n",
        "\n",
        " - Language Model Used:\n",
        "\t - DistilBERT this is a smaller transformer model as compared to BERT or Roberta. It is created by process of distillation applied to Bert. \n",
        "\t - [Blog-Post](https://medium.com/huggingface/distilbert-8cf3380435b5)\n",
        "\t - [Research Paper](https://arxiv.org/abs/1910.01108)\n",
        "     - [Documentation for python](https://huggingface.co/transformers/model_doc/distilbert.html)\n",
        "\n",
        "\n",
        " - Hardware Requirements:\n",
        "\t - Python 3.6 and above\n",
        "\t - Pytorch, Transformers and All the stock Python ML Libraries\n",
        "\t - GPU enabled setup \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCTBrSCAXBHT"
      },
      "source": [
        "<a id='section01'></a>\n",
        "### Importing Python Libraries and preparing the environment\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuMlXT80GAMK"
      },
      "outputs": [],
      "source": [
        "# Importing the libraries needed\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "import linecache\n",
        "from pathlib import Path\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import pandas as pd\n",
        "import gc\n",
        "gc.enable()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQMKTZ4ARk12"
      },
      "outputs": [],
      "source": [
        "# Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNwhypz3XBHT"
      },
      "source": [
        "<a id='section02'></a>\n",
        "### Importing and Pre-Processing data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Connect to google drive"
      ],
      "metadata": {
        "id": "z7ojooFKXJkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cP_AL-BzXkKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raV4ms5IXBHT"
      },
      "source": [
        "2. Copy and unzip data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/industry_data/test_small.ndjson.gz . && gzip -d test_small.ndjson.gz\n",
        "!cp /content/drive/MyDrive/industry_data/train_small.ndjson.gz . && gzip -d train_small.ndjson.gz"
      ],
      "metadata": {
        "id": "BPgOSEQMXy7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Get categories"
      ],
      "metadata": {
        "id": "ZZ3rWgTNZAHo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMeSeJJaXBHT"
      },
      "outputs": [],
      "source": [
        "data_path = Path('.')\n",
        "test_path = data_path/'train_small.ndjson'\n",
        "with test_path.open(\"r\", encoding=\"utf-8\") as file:\n",
        "    test_data = [json.loads(line) for line in file]\n",
        "categories = pd.DataFrame(test_data)\n",
        "categories = categories[['industry_label', 'industry']].sort_values('industry_label').drop_duplicates().reset_index(drop=True)\n",
        "categories = categories.reset_index().set_index('industry')\n",
        "del test_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categories"
      ],
      "metadata": {
        "id": "VOTmKinhZM-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "s4YhYxW1XBHV"
      },
      "source": [
        "<a id='section03'></a>\n",
        "### Preparing the Dataset and Dataloader\n",
        "\n",
        "We will start with defining few key variables that will be used later during the training/fine tuning stage.\n",
        "Followed by creation of Dataset class - This defines how the text is pre-processed before sending it to the neural network. We will also define the Dataloader that will feed  the data in batches to the neural network for suitable training and processing. \n",
        "Dataset and Dataloader are constructs of the PyTorch library for defining and controlling the data pre-processing and its passage to neural network. For further reading into Dataset and Dataloader read the [docs at PyTorch](https://pytorch.org/docs/stable/data.html)\n",
        "\n",
        "#### *NdjsonDataset* Dataset Class\n",
        "- This class is defined to generate tokenized output that is used by the DistilBERT model for training. \n",
        "\n",
        "#### Dataloader\n",
        "- Dataloader is used to for creating training and validation dataloader that load data to the neural network in a defined manner. This is needed because all the data from the dataset cannot be loaded to the memory at once, hence the amount of dataloaded to the memory and then passed to the neural network needs to be controlled.\n",
        "- This control is achieved using the parameters such as `batch_size` and `max_len`.\n",
        "- Training and Validation dataloaders are used in the training and validation part of the flow respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrBr2YesGdO_"
      },
      "outputs": [],
      "source": [
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 512\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "VALID_BATCH_SIZE = 2\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xJX9xRiXBHV"
      },
      "outputs": [],
      "source": [
        "class NdjsonDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, filepath, categories, tokenizer, max_len, n_lines=None):\n",
        "        super(NdjsonDataset).__init__()\n",
        "        self.filename = filepath.as_posix()\n",
        "        self.categories = categories\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        with filepath.open(\"r\", encoding=\"utf-8\") as file:\n",
        "            self.n_lines = n_lines or sum(1 for line in file)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.n_lines\n",
        "            \n",
        "    def __getitem__(self, idx):\n",
        "        line = json.loads(linecache.getline(self.filename, idx+1))\n",
        "        industry = line['industry'] \n",
        "        plainline = BeautifulSoup(line['html'], 'html.parser').get_text()\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            plainline,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.categories.at[industry, 'index'], dtype=torch.long)\n",
        "        } "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clWTsuApXBHV"
      },
      "outputs": [],
      "source": [
        "# Creating the dataset and dataloader for the neural network\n",
        "# How would you creat a validation dataset?\n",
        "\n",
        "data_path = Path('.')\n",
        "\n",
        "training_set = NdjsonDataset(filepath=data_path/'train_small.ndjson', \n",
        "                        categories=categories, \n",
        "                        tokenizer=tokenizer,\n",
        "                        max_len=MAX_LEN,\n",
        "                        n_lines=25185)\n",
        "\n",
        "testing_set = NdjsonDataset(filepath=data_path/'test_small.ndjson', \n",
        "                        categories=categories, \n",
        "                        tokenizer=tokenizer,\n",
        "                        max_len=MAX_LEN,\n",
        "                        n_lines=8396)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1BgA1CkQSYa"
      },
      "outputs": [],
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mK__ySUXBHW"
      },
      "source": [
        "<a id='section04'></a>\n",
        "### Creating the Neural Network for Fine Tuning\n",
        "\n",
        "#### Neural Network\n",
        " - We will be creating a neural network with the `DistillBERTClass`. \n",
        " - This network will have the DistilBERT Language model followed by a `dropout` and finally a `Linear` layer to obtain the final outputs. \n",
        " - The data will be fed to the DistilBERT Language model as defined in the dataset. \n",
        " - Final layer outputs is what will be compared to the `encoded category` to determine the accuracy of models prediction. \n",
        " - We will initiate an instance of the network called `model`. This instance will be used for training and then to save the final trained model for future inference. \n",
        " \n",
        "#### Loss Function and Optimizer\n",
        " - `Loss Function` and `Optimizer` and defined in the next cell.\n",
        " - The `Loss Function` is used the calculate the difference in the output created by the model and the actual output. \n",
        " - `Optimizer` is used to update the weights of the neural network to improve its performance.\n",
        " \n",
        "#### Further Reading\n",
        "- You can refer to my [Pytorch Tutorials](https://github.com/abhimishra91/pytorch-tutorials) to get an intuition of Loss Function and Optimizer.\n",
        "- [Pytorch Documentation for Loss Function](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "- [Pytorch Documentation for Optimizer](https://pytorch.org/docs/stable/optim.html)\n",
        "- Refer to the links provided on the top of the notebook to read more about DistiBERT. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nzm25z5HXBHW"
      },
      "outputs": [],
      "source": [
        "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
        "\n",
        "class DistillBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DistillBERTClass, self).__init__()\n",
        "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, len(categories))\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "LVcetSqxXBHX"
      },
      "outputs": [],
      "source": [
        "model = DistillBERTClass()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gUTdlTxXBHX"
      },
      "outputs": [],
      "source": [
        "# Creating the loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dhmzEnKXBHX"
      },
      "source": [
        "<a id='section05'></a>\n",
        "### Fine Tuning the Model\n",
        "\n",
        "After all the effort of loading and preparing the data and datasets, creating the model and defining its loss and optimizer. This is probably the easier steps in the process. \n",
        "\n",
        "Here we define a training function that trains the model on the training dataset created above, specified number of times (EPOCH), An epoch defines how many times the complete data will be passed through the network. \n",
        "\n",
        "Following events happen in this function to fine tune the neural network:\n",
        "- The dataloader passes data to the model based on the batch size. \n",
        "- Subsequent output from the model and the actual category are compared to calculate the loss. \n",
        "- Loss value is used to optimize the weights of the neurons in the network.\n",
        "- After every 100 steps the loss value is printed in the console."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlSFQ3TGXBHX"
      },
      "outputs": [],
      "source": [
        "# Function to calcuate the accuracy of the model\n",
        "\n",
        "def calcuate_accu(big_idx, targets):\n",
        "    n_correct = (big_idx==targets).sum().item()\n",
        "    return n_correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fL2BMY3vXBHX"
      },
      "outputs": [],
      "source": [
        "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
        "\n",
        "def train(epoch):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model.train()\n",
        "    for _,data in enumerate(training_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(ids, mask)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calcuate_accu(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "        \n",
        "        if _%100==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            accu_step = (n_correct*100)/nb_tr_examples \n",
        "            print(f\"Training Loss per 100 steps: {loss_step}\")\n",
        "            print(f\"Training Accuracy per 100 steps: {accu_step}\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # # When using GPU\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "    return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmYngH8_XBHX"
      },
      "outputs": [],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjmMzuoQXBHX"
      },
      "source": [
        "<a id='section06'></a>\n",
        "### Validating the Model (not tested)\n",
        "\n",
        "During the validation stage we pass the unseen data(Testing Dataset) to the model. This step determines how good the model performs on the unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiw5tavBXBHX"
      },
      "outputs": [],
      "source": [
        "def valid(model, testing_loader):\n",
        "    model.eval()\n",
        "    n_correct = 0; n_wrong = 0; total = 0\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(testing_loader, 0):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.long)\n",
        "            outputs = model(ids, mask).squeeze()\n",
        "            loss = loss_function(outputs, targets)\n",
        "            tr_loss += loss.item()\n",
        "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "            n_correct += calcuate_accu(big_idx, targets)\n",
        "\n",
        "            nb_tr_steps += 1\n",
        "            nb_tr_examples+=targets.size(0)\n",
        "            \n",
        "            if _%5000==0:\n",
        "                loss_step = tr_loss/nb_tr_steps\n",
        "                accu_step = (n_correct*100)/nb_tr_examples\n",
        "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
        "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
        "    \n",
        "    return epoch_accu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUYk7tNzXBHY"
      },
      "outputs": [],
      "source": [
        "print('This is the validation section to print the accuracy and see how it performs')\n",
        "print('Here we are leveraging on the dataloader crearted for the validation dataset, the approcah is using more of pytorch')\n",
        "\n",
        "acc = valid(model, testing_loader)\n",
        "print(\"Accuracy on test data = %0.2f%%\" % acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9Y_K5o7XBHY"
      },
      "source": [
        "<a id='section07'></a>\n",
        "### Saving the Trained Model Artifacts for inference\n",
        "\n",
        "This is the final step in the process of fine tuning the model. \n",
        "\n",
        "The model and its vocabulary are saved locally. These files are then used in the future to make inference on new inputs of news headlines.\n",
        "\n",
        "Please remember that a trained neural network is only useful when used in actual inference after its training. \n",
        "\n",
        "In the lifecycle of an ML projects this is only half the job done. We will leave the inference of these models for some other day. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmRDJPFcXBHY"
      },
      "outputs": [],
      "source": [
        "# Saving the files for re-use\n",
        "\n",
        "output_model_file = './models/pytorch_distilbert_news.bin'\n",
        "output_vocab_file = './models/vocab_distilbert_news.bin'\n",
        "\n",
        "model_to_save = model\n",
        "torch.save(model_to_save, output_model_file)\n",
        "tokenizer.save_vocabulary(output_vocab_file)\n",
        "\n",
        "print('All files saved')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Slideshow",
    "colab": {
      "collapsed_sections": [],
      "name": "10_Project_Questions.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "pds",
      "language": "python",
      "name": "pds"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "rise": {
      "enable_chalkboard": true,
      "overlay": "<div class='background'></div><div class='header'>WS 21/22</br>PDS</div><div class='logo'></div><div class='bar'></div>",
      "scroll": true,
      "slideNumber": "h.v"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}