{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Practical Data Science 21/22*\n",
    "# Programming Assignment 2 - Predicting Video Game Sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this programming assignment you need to apply your new (or refreshed) machine learning knowledge. You will need to create a modeling pipeline training and evaluating a machine learning model build on several numeric as well as categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Dataset\n",
    "\n",
    "You are provided with a dataset containing a list of video games with sales greater than 100.000 copies. Your task is to build a model predicting the yearly global sales (column ``Global_Sales``) of a video game leveraging the available features.\n",
    "\n",
    "To help you get started, the following blocks of code import the dataset using pandas: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Year_of_Release</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Global_Sales</th>\n",
       "      <th>Critic_Score</th>\n",
       "      <th>Critic_Count</th>\n",
       "      <th>User_Score</th>\n",
       "      <th>User_Count</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wii Sports</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>82.53</td>\n",
       "      <td>76.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Super Mario Bros.</td>\n",
       "      <td>NES</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>Platform</td>\n",
       "      <td>40.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mario Kart Wii</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>Racing</td>\n",
       "      <td>35.52</td>\n",
       "      <td>82.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>709.0</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wii Sports Resort</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>32.77</td>\n",
       "      <td>80.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pokemon Red/Pokemon Blue</td>\n",
       "      <td>GB</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>Role-Playing</td>\n",
       "      <td>31.37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Name Platform  Year_of_Release         Genre  \\\n",
       "0                Wii Sports      Wii           2006.0        Sports   \n",
       "1         Super Mario Bros.      NES           1985.0      Platform   \n",
       "2            Mario Kart Wii      Wii           2008.0        Racing   \n",
       "3         Wii Sports Resort      Wii           2009.0        Sports   \n",
       "4  Pokemon Red/Pokemon Blue       GB           1996.0  Role-Playing   \n",
       "\n",
       "   Global_Sales  Critic_Score  Critic_Count  User_Score  User_Count Rating  \n",
       "0         82.53          76.0          51.0         8.0       322.0      E  \n",
       "1         40.24           NaN           NaN         NaN         NaN    NaN  \n",
       "2         35.52          82.0          73.0         8.3       709.0      E  \n",
       "3         32.77          80.0          73.0         8.0       192.0      E  \n",
       "4         31.37           NaN           NaN         NaN         NaN    NaN  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'https://raw.githubusercontent.com/NikoStein/pds_data/main/data/video_game_sales.csv'\n",
    "game_sales_data = pd.read_csv(data_path)\n",
    "game_sales_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Dataset\n",
    "\n",
    "Before you can get started training a machine learning model you will have to split the dataframe into features and the target variable (try to use as many features as possible):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Platform', 'Year_of_Release', 'Genre', 'Global_Sales', 'Critic_Score',\n",
       "       'Critic_Count', 'User_Score', 'User_Count', 'Rating'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_sales_data.set_index('Name', inplace=True)\n",
    "game_sales_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name\n",
      "Wii Sports                  82.53\n",
      "Super Mario Bros.           40.24\n",
      "Mario Kart Wii              35.52\n",
      "Wii Sports Resort           32.77\n",
      "Pokemon Red/Pokemon Blue    31.37\n",
      "Name: Global_Sales, dtype: float64\n",
      "                         Platform  Year_of_Release         Genre  \\\n",
      "Name                                                               \n",
      "Wii Sports                    Wii           2006.0        Sports   \n",
      "Super Mario Bros.             NES           1985.0      Platform   \n",
      "Mario Kart Wii                Wii           2008.0        Racing   \n",
      "Wii Sports Resort             Wii           2009.0        Sports   \n",
      "Pokemon Red/Pokemon Blue       GB           1996.0  Role-Playing   \n",
      "\n",
      "                          Critic_Score  Critic_Count  User_Score  User_Count  \\\n",
      "Name                                                                           \n",
      "Wii Sports                        76.0          51.0         8.0       322.0   \n",
      "Super Mario Bros.                  NaN           NaN         NaN         NaN   \n",
      "Mario Kart Wii                    82.0          73.0         8.3       709.0   \n",
      "Wii Sports Resort                 80.0          73.0         8.0       192.0   \n",
      "Pokemon Red/Pokemon Blue           NaN           NaN         NaN         NaN   \n",
      "\n",
      "                         Rating  \n",
      "Name                             \n",
      "Wii Sports                    E  \n",
      "Super Mario Bros.           NaN  \n",
      "Mario Kart Wii                E  \n",
      "Wii Sports Resort             E  \n",
      "Pokemon Red/Pokemon Blue    NaN  \n"
     ]
    }
   ],
   "source": [
    "y = game_sales_data['Global_Sales']\n",
    "X = game_sales_data.drop('Global_Sales', axis=1)\n",
    "print(y.head())\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will have to create a train-test split in order to be able to evaluate your models. Use 80\\% of the data for training and 20\\% for evaluation (take a look at the sklearn [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to identify the relevant parameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, \n",
    "                                                  train_size=0.8, \n",
    "                                                  random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing missing values\n",
    "If you inspect your training data you will find that some of the variables have missing values. Use the ``SimpleImputer`` to replace missing values in numerical columns with the column mean and missing values in categorical columns with the most frequent value (take a look at the SimpleImputer [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) to identify the relevant parameters). You can decide if you want to use the simple or the advanced imputation strategy (or just try both)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Platform            object\n",
       "Year_of_Release    float64\n",
       "Genre               object\n",
       "Critic_Score       float64\n",
       "Critic_Count       float64\n",
       "User_Score         float64\n",
       "User_Count         float64\n",
       "Rating              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [col for col in train_X.columns if train_X[col].dtype == 'float64']\n",
    "cat_cols = [col for col in train_X.columns if train_X[col].dtype == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "train_X_num_imputed = pd.DataFrame(num_imputer.fit_transform(train_X[num_cols]), \n",
    "                                   columns=num_cols, index=train_X.index)\n",
    "val_X_num_imputed = pd.DataFrame(num_imputer.transform(val_X[num_cols]), \n",
    "                                   columns=num_cols, index=val_X.index)\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "train_X_cat_imputed = pd.DataFrame(cat_imputer.fit_transform(train_X[cat_cols]), \n",
    "                                   columns=cat_cols, index=train_X.index)\n",
    "val_X_cat_imputed = pd.DataFrame(cat_imputer.transform(val_X[cat_cols]), \n",
    "                                   columns=cat_cols, index=val_X.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical variables\n",
    "\n",
    "Prior to training your model you will have to encode the categorical variables. Inspect all categorical variables and use the ``LabelEncoder`` or the ``OneHotEncoder`` where appropriate. Remember that you have to combine the numerical as well as the label encoded and the one hot encoded dataframes at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: 31\n",
      "Genre: 12\n",
      "Rating: 8\n"
     ]
    }
   ],
   "source": [
    "for cat in cat_cols:\n",
    "    print(\"{}: {}\".format(cat, game_sales_data[cat].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "E       3989\n",
       "T       2958\n",
       "M       1561\n",
       "E10+    1419\n",
       "EC         8\n",
       "K-A        3\n",
       "RP         3\n",
       "AO         1\n",
       "Name: Rating, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_sales_data['Rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder = OrdinalEncoder()\n",
    "train_X_cat_label = pd.DataFrame(ordinal_encoder.fit_transform(train_X_cat_imputed[[\"Platform\", 'Genre']]),\n",
    "                                 columns=[\"Platform\", 'Genre'], \n",
    "                                 index=train_X_cat_imputed.index)\n",
    "val_X_cat_label = pd.DataFrame(ordinal_encoder.transform(val_X_cat_imputed[[\"Platform\", 'Genre']]),\n",
    "                                 columns=[\"Platform\", 'Genre'], \n",
    "                                 index=val_X_cat_imputed.index)\n",
    "\n",
    "\n",
    "ohe_encoder = OneHotEncoder(sparse=False)\n",
    "train_X_cat_ohe = pd.DataFrame(ohe_encoder.fit_transform(train_X_cat_imputed[['Rating']]),\n",
    "                                 index=train_X_cat_imputed.index, columns=ohe_encoder.get_feature_names_out())\n",
    "val_X_cat_ohe = pd.DataFrame(ohe_encoder.transform(val_X_cat_imputed[['Rating']]),\n",
    "                                 index=val_X_cat_imputed.index, columns=ohe_encoder.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.concat([train_X_num_imputed, train_X_cat_label, train_X_cat_ohe], axis=1)\n",
    "val_X = pd.concat([val_X_num_imputed, val_X_cat_label, val_X_cat_ohe], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Now our dataset should be ready and we can train a predictive model. Train a Decision Tree as well as a Random Forest and compare the in-sample as well as the out-of-sample performance of both models usinge the mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error# Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model_rf = RandomForestRegressor(n_estimators=100, random_state=1)\n",
    "    model_rf.fit(X_train, y_train)\n",
    "    preds_rf = model_rf.predict(X_valid)\n",
    "    model_dt = DecisionTreeRegressor(random_state=1)\n",
    "    model_dt.fit(X_train, y_train)\n",
    "    preds_dt = model_dt.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds_rf), mean_absolute_error(y_valid, preds_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_rf, oos_dt = score_dataset(train_X, val_X, train_y, val_y)\n",
    "is_rf, is_dt = score_dataset(train_X, train_X, train_y, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-sample\n",
      "Random Forest: 0.4499542782969004\n",
      "Decicion Tree\" 0.5214266370937557\n",
      "------------------------------\n",
      "In-sample\n",
      "Random Forest: 0.23666406742477258\n",
      "Decicion Tree\" 0.12801702497592565\n"
     ]
    }
   ],
   "source": [
    "print('Out-of-sample\\nRandom Forest: {}\\nDecicion Tree\" {}'.format(oos_rf, oos_dt))\n",
    "print('------------------------------')\n",
    "print('In-sample\\nRandom Forest: {}\\nDecicion Tree\" {}'.format(is_rf, is_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(X, y, \n",
    "                                                  train_size=0.8, \n",
    "                                                  random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE using the complete pipeline: 0.44182052452037074\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing numerical columns\n",
    "numerical_transformer = SimpleImputer(strategy='mean')\n",
    "\n",
    "## Preprocessing categorical columns\n",
    "\n",
    "# Ordinal Encoder\n",
    "categorical_transformer_ordinal = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')) \n",
    "])\n",
    "\n",
    "# One hot encoder\n",
    "categorical_transformer_ohe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')) \n",
    "])\n",
    "\n",
    "# Bundle the preprocessors\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_transformer, num_cols),\n",
    "    ('cat_ordinal', categorical_transformer_ordinal, [\"Platform\", 'Genre']),\n",
    "    ('cat_ohe', categorical_transformer_ohe, ['Rating'])\n",
    "])\n",
    "\n",
    "# Create model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=1)\n",
    "\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "complete_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Preprocess the raw training data and fit the model\n",
    "complete_pipeline.fit(train_X, train_y)\n",
    "\n",
    "# Preprocess the raw validation data and make predictions\n",
    "preds = complete_pipeline.predict(val_X)\n",
    "\n",
    "# Evaluate the model\n",
    "score = mean_absolute_error(val_y, preds)\n",
    "print(\"MAE using the complete pipeline: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the Model\n",
    "\n",
    "Having successfully trained a model, your next task is to improve its performance. Try different advanced feature engineering techniques and see if they are able to improve your model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install category_encoders\n",
    "from category_encoders import CatBoostEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(X, y, \n",
    "                                                  train_size=0.8, \n",
    "                                                  random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [col for col in train_X.columns if train_X[col].dtype == 'float64']\n",
    "cat_cols = [col for col in train_X.columns if train_X[col].dtype == 'object']\n",
    "\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "train_X_num_imputed = pd.DataFrame(num_imputer.fit_transform(train_X[num_cols]), \n",
    "                                   columns=num_cols, index=train_X.index)\n",
    "val_X_num_imputed = pd.DataFrame(num_imputer.transform(val_X[num_cols]), \n",
    "                                   columns=num_cols, index=val_X.index)\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "train_X_cat_imputed = pd.DataFrame(cat_imputer.fit_transform(train_X[cat_cols]), \n",
    "                                   columns=cat_cols, index=train_X.index)\n",
    "val_X_cat_imputed = pd.DataFrame(cat_imputer.transform(val_X[cat_cols]), \n",
    "                                   columns=cat_cols, index=val_X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_encoder = CatBoostEncoder()\n",
    "\n",
    "train_X_catboostenc = catboost_encoder.fit_transform(train_X_cat_imputed, train_y)\n",
    "val_X_catboostenc = catboost_encoder.transform(val_X_cat_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.concat([train_X_num_imputed, train_X_catboostenc], axis=1)\n",
    "val_X = pd.concat([val_X_num_imputed, val_X_catboostenc], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_rf, oos_dt = score_dataset(train_X, val_X, train_y, val_y)\n",
    "is_rf, is_dt = score_dataset(train_X, train_X, train_y, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-sample\n",
      "Random Forest: 0.4475071492671253\n",
      "Decicion Tree\" 0.5218905174992521\n",
      "------------------------------\n",
      "In-sample\n",
      "Random Forest: 0.17164996259724713\n",
      "Decicion Tree\" 2.0243631227736527e-20\n"
     ]
    }
   ],
   "source": [
    "print('Out-of-sample\\nRandom Forest: {}\\nDecicion Tree\" {}'.format(oos_rf, oos_dt))\n",
    "print('------------------------------')\n",
    "print('In-sample\\nRandom Forest: {}\\nDecicion Tree\" {}'.format(is_rf, is_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(X, y, \n",
    "                                                  train_size=0.8, \n",
    "                                                  random_state = 0)\n",
    "\n",
    "# Find all columns with missing values:\n",
    "cols_with_missing = train_X.columns.values[train_X.isna().sum() > 0]\n",
    "\n",
    "# Make new columns indicating what will be imputed\n",
    "for col in cols_with_missing:\n",
    "    train_X[col + '_was_missing'] = train_X[col].isnull()\n",
    "    val_X[col + '_was_missing'] = val_X[col].isnull()\n",
    "    \n",
    "    \n",
    "num_cols = [col for col in train_X.columns if train_X[col].dtype == 'float64']\n",
    "cat_cols = [col for col in train_X.columns if train_X[col].dtype == 'object']\n",
    "bool_cols = [col for col in train_X.columns if train_X[col].dtype == 'bool']\n",
    "    \n",
    "# Imputation\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "train_X_num_imputed = pd.DataFrame(num_imputer.fit_transform(train_X[num_cols]),\n",
    "                              columns=num_cols, index = train_X.index)\n",
    "val_X_num_imputed = pd.DataFrame(num_imputer.transform(val_X[num_cols]),\n",
    "                            columns=num_cols, index = val_X.index)\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "train_X_cat_imputed = pd.DataFrame(cat_imputer.fit_transform(train_X[cat_cols]), \n",
    "                                   columns=cat_cols, index=train_X.index)\n",
    "val_X_cat_imputed = pd.DataFrame(cat_imputer.transform(val_X[cat_cols]), \n",
    "                                   columns=cat_cols, index=val_X.index)\n",
    "\n",
    "train_X = pd.concat([train_X_num_imputed, train_X_cat_imputed, train_X[bool_cols]], axis=1)\n",
    "val_X = pd.concat([val_X_num_imputed, val_X_cat_imputed, val_X[bool_cols]], axis=1)\n",
    "\n",
    "# Encodings\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "train_X_cat_label = pd.DataFrame(ordinal_encoder.fit_transform(train_X_cat_imputed[[\"Platform\", 'Genre']]),\n",
    "                                 columns=[\"Platform\", 'Genre'], \n",
    "                                 index=train_X_cat_imputed.index)\n",
    "val_X_cat_label = pd.DataFrame(ordinal_encoder.transform(val_X_cat_imputed[[\"Platform\", 'Genre']]),\n",
    "                                 columns=[\"Platform\", 'Genre'], \n",
    "                                 index=val_X_cat_imputed.index)\n",
    "\n",
    "\n",
    "ohe_encoder = OneHotEncoder(sparse=False)\n",
    "train_X_cat_ohe = pd.DataFrame(ohe_encoder.fit_transform(train_X_cat_imputed[['Rating']]),\n",
    "                                 index=train_X_cat_imputed.index, columns=ohe_encoder.get_feature_names_out())\n",
    "val_X_cat_ohe = pd.DataFrame(ohe_encoder.transform(val_X_cat_imputed[['Rating']]),\n",
    "                                 index=val_X_cat_imputed.index, columns=ohe_encoder.get_feature_names_out())\n",
    "\n",
    "train_X = pd.concat([train_X_num_imputed, train_X_cat_label, train_X_cat_ohe], axis=1)\n",
    "val_X = pd.concat([val_X_num_imputed, val_X_cat_label, val_X_cat_ohe], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-sample\n",
      "Random Forest: 0.4499542782969004\n",
      "Decicion Tree\" 0.5214266370937557\n",
      "------------------------------\n",
      "In-sample\n",
      "Random Forest: 0.23666406742477258\n",
      "Decicion Tree\" 0.12801702497592565\n"
     ]
    }
   ],
   "source": [
    "oos_rf, oos_dt = score_dataset(train_X, val_X, train_y, val_y)\n",
    "is_rf, is_dt = score_dataset(train_X, train_X, train_y, train_y)\n",
    "\n",
    "print('Out-of-sample\\nRandom Forest: {}\\nDecicion Tree\" {}'.format(oos_rf, oos_dt))\n",
    "print('------------------------------')\n",
    "print('In-sample\\nRandom Forest: {}\\nDecicion Tree\" {}'.format(is_rf, is_dt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
